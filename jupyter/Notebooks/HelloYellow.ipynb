{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "856b0e94-09bd-41c9-ac0f-604889d9dfc2",
   "metadata": {},
   "source": [
    "# Stamus Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e469a548-06e2-4c9e-8266-478a27da0016",
   "metadata": {},
   "source": [
    "## SELKS\n",
    "\n",
    "* https://www.stamus-networks.com/selks\n",
    "\n",
    "SELKS stands for Suricata, Elasticsearch, Logstash, Kibana, Scirius. It also includes Arkime, Evebokx and CyberChef, though I suppose there the acronym was already set. It is a fully open-source and free to use. One might rightfully assume that Stamus Security Platform, our commertial product, is basically SELKS with support. But that's actually not correct. We treat SELKS and SSP as totally separate products aimed at different audiences. \n",
    "\n",
    "SELKS is a beginner-friendly Suricata stack for enthusiasts, open-source aficionados, and for smaller deployments. It is an suiteable for small to medium sized organizations, or for PoC setups in larger organizations. It also works well for forensics and trainings. By comparison, SSP is mostly used by large enterprises that often monitor multiple 10 or 40 gigabit links. We also offer smaller probes like 100 megabit and 1 gigabit, but our most popular solution is in 10 gigabit range. We are currently also working on 100 gigabit probe.\n",
    "\n",
    "To sum it up:\n",
    "\n",
    "* SELKS is a single-box distribution with no probe management, whereas SSP comprises of management server (SCS) and any number of licenced probes;\n",
    "* SSP probes are pre-optimized for specific traffic sizes. We licence hardware in addition to software, meaning customer receives optimized hardware appliance for traffic capture;\n",
    "* SELKS deployment size is basically up to the user. Capture hardware, suricata and elasticsearch optimizations need to be customized per deployment;\n",
    "* SSP probe is not just a server running Suricata. It's an appliance with custom Suricata build and event post-processing pipeline. By comparison, SELKS uses logstash to ingest EVE JSON logs from local open-source Suricata into elasticsearch with no additional processing;\n",
    "* Scirius hunting dashboard is basically the same, but SSP has additional filtersets;\n",
    "* SSP has a backend threat intel feed to highlight and contextualize high fidelity alerts;\n",
    "* No post-processing means SELKS is lacking advanced analytics features, such as DoC (declaration of compromise) contextualization, host fingerprinting, beacon detection, etc;\n",
    "* SELKS ruleset management is functionally identical to SSP, but the latter packages ET PRO with the product (if licenced accordingly);\n",
    "* We also have original Stamus rulesets for threat hunting. They are mostly open-source and can be enabled in SELKS, but we provide propiretary versions that are optimized for SSP;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88225160-8444-45af-ab5d-c0eedfd8ff7f",
   "metadata": {},
   "source": [
    "### SELKS on Docker\n",
    "\n",
    "SELKS was originally envisioned as security distribution (sales like to use the term \"turnkey solution\"). Similar to Security Onion, but focused on Suricata data. That is still the case. We have major releases that can be installed as ISO images. But technology marches on and anyone can now set up latest and greatest codebase as a docker stack.\n",
    "\n",
    "Firstly, we need to clone the SELKS repository from github.\n",
    "\n",
    "```\n",
    "git clone https://github.com/StamusNetworks/SELKS.git\n",
    "```\n",
    "\n",
    "Then navigate to `docker` subfolder in the newly cloned repository.\n",
    "\n",
    "```\n",
    "cd SELKS/docker\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b9263c-3f78-483a-a025-aeabaf15b77f",
   "metadata": {},
   "source": [
    "### (Optional) - set up a clean vagrant env\n",
    "\n",
    "While the stack can be spun up on bere metal, I suggest a clean environment for testing. Vagrant is a useful tool quickly spinning one up. Note that we generally suggest at least 8 gigabytes of memory for this host. If you want to use [vagrant](https://developer.hashicorp.com/vagrant/downloads?product_intent=vagrant), then simply add this content into `Vagrantfile` in `SELKS/docker` folder.\n",
    "\n",
    "```ruby\n",
    "$provision = <<-SCRIPT\n",
    "  export DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "  apt-get update\n",
    "SCRIPT\n",
    "\n",
    "NAME = 'selks'.freeze\n",
    "CPU = 4\n",
    "MEM = 8192\n",
    "\n",
    "Vagrant.configure(2) do |config|\n",
    "  config.vm.define NAME do |box|\n",
    "    box.vm.box = 'debian/bullseye64'\n",
    "    box.vm.hostname = NAME\n",
    "    box.vm.network :private_network, ip: '192.168.56.10'\n",
    "    box.vm.synced_folder '.', '/selks', type: 'rsync', rsync__exclude: '.git/'\n",
    "    box.vm.provider :virtualbox do |vb|\n",
    "      vb.customize ['modifyvm', :id, '--memory', MEM]\n",
    "      vb.customize ['modifyvm', :id, '--cpus', CPU]\n",
    "    end\n",
    "    box.vm.provider 'libvirt' do |v, _|\n",
    "      v.cpus = CPU\n",
    "      v.memory = MEM\n",
    "    end\n",
    "    box.vm.provision 'shell', inline: $provision\n",
    "  end\n",
    "end\n",
    "```\n",
    "\n",
    "Then start the vagrant provisioner.\n",
    "\n",
    "```\n",
    "vagrant up\n",
    "```\n",
    "\n",
    "SSH into the box.\n",
    "\n",
    "```\n",
    "vagrant ssh\n",
    "```\n",
    "\n",
    "And navigate to the folder where selks source was synced. Note that `synced_folder` must not be a file share. Selks setup script will set up and mount data directories into docker containers. So it will run into permission issues if we mount the folder. Example vagrantfile uses `rsync` provisioner instead to bypass the issue.\n",
    "\n",
    "```\n",
    "cd /selks\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef29c5-0ef6-41f8-bc38-41000365cd86",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Virtual capture interface\n",
    "\n",
    "\n",
    "Before setting up the SELKS instance, we should create a dummy interface for Suricata to listen on. Normally you'd want to capture data off a real NIC, but this setup is meant for investigating malware PCAPs. So, it's better to create a psuedo interface for traffic replay.\n",
    "\n",
    "```\n",
    "sudo ip link add tppdummy0 type dummy\n",
    "sudo ip link set tppdummy0 up \n",
    "sudo ip link set dev tppdummy0 mtu 9000\n",
    "```\n",
    "\n",
    "**These commands are not persistent!** They need to be reapplied after reboot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19342e70-4cf6-4fbb-9b4f-20e335b85388",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Once done, launch the easy-setup script. Normally it would be interactive, but we'll use a noninteractive version to quickly set up a playground instance. **Note that this non-interactive version assumes a clean box and will install docker for you**.\n",
    "\n",
    "```\n",
    "sudo ./easy-setup.sh  --non-interactive -i tppdummy0 --iA --es-memory 4G --ls-memory 2G\n",
    "```\n",
    "\n",
    "It's mostly just a wrapper to set up dependencies and `.env` file that docker-compose would then be using. It will also pull docker images. Feel free to experiment with other options. If you feel proficient, then `.env` can also me modified directly.\n",
    "\n",
    "In fact, we should actually do it since the default image is pretty old and does not have new frontend design that matches with SSP. We can change that easily by defining a much newer image. Just add this line at the end of the `.env` file.\n",
    "\n",
    "```\n",
    "SCIRIUS_VERSION=master-20231006\n",
    "```\n",
    "\n",
    "Finally start up the docker stack.\n",
    "\n",
    "```\n",
    "sudo -E docker compose up -d\n",
    "```\n",
    "\n",
    "Finally, visit the external IP port 443 of whatever server you chose to deploy SELKS. Default user is `selks_user`. Password is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6291e1-bd0e-4b12-a5b2-cb3732eb802e",
   "metadata": {},
   "source": [
    "### PCAP replay\n",
    "\n",
    "Once the stack is up and running, we can use the newly created dummy interface for traffic replay. But before we do, we need a PCAP file. A good source is [Malware Traffic Analysis](https://malware-traffic-analysis.net). Feel free to pick a case study you'd like to investigate. Each case usually has a zipped PCAP file. Standard practice is to password protect those files to avoid having them trigger IDS systems. Password is always `infected`.\n",
    "\n",
    "Some good samples are:\n",
    "* [Web server scanning traffic](https://malware-traffic-analysis.net/2022/01/03/2022-01-01-thru-03-server-activity-with-log4j-attempts.pcap.zip);\n",
    "* [Malware infecting workstation and then compromises DC](https://malware-traffic-analysis.net/2020/03/04/2020-03-04-Trickbot-spreads-from-client-to-DC.pcap.zip)\n",
    "\n",
    "Then download a sample and unzip it.\n",
    "\n",
    "```\n",
    "wget \"https://malware-traffic-analysis.net/2022/01/03/2022-01-01-thru-03-server-activity-with-log4j-attempts.pcap.zip\"\n",
    "unzip 2022-01-01-thru-03-server-activity-with-log4j-attempts.pcap.zip\n",
    "```\n",
    "\n",
    "And start the replay. Note that most MTA PCAPs are really small, so adjust the `pps` (packets per second) accordingly. Most files will be processed really fast and scirius might not be able to draw out a timeline. Lower value might be better for smaller files. Simply don't make it so small you need to wait too long. Your mileage might vary.\n",
    "\n",
    "```\n",
    "sudo apt install tcpreplay\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "sudo tcpreplay --pps 200 -i tppdummy0 2022-01-01-thru-03-server-activity-with-log4j-attempts.pcap\n",
    "```\n",
    "\n",
    "Hunting dashboard should then be populated by data. Could be that some cases do not trigger many (or any) rules. That does not mean we don't have data! Simply peek into kibana or evebox instead.\n",
    "\n",
    "To reset the state before replaying another PCAP, use a cleanup script. It will wipe all events from elasticsearch.\n",
    "\n",
    "```\n",
    "sudo ./scripts/cleanup.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592c8872-410a-4efa-a4b2-62622afb6758",
   "metadata": {
    "tags": []
   },
   "source": [
    "## XS Hunting with SSP\n",
    "\n",
    "Here are a few case studies to explore with SSP and XS data. General hunting flow is:\n",
    "\n",
    "* apply filters to find interesting events;\n",
    "* use filtersets to streamline the process or when unsure what to do;\n",
    "* investigate individual events or interesting metadata values;\n",
    "* use host insights for pivoting;\n",
    "\n",
    "### Executable downloads via Powershell\n",
    "\n",
    "![Powershell filterset](img/hy-powershell-filters.png)\n",
    "\n",
    "### HTTP user-agents\n",
    "\n",
    "![Suspicious user agents](img/hy-one-word-ua.png)\n",
    "\n",
    "### Discovery\n",
    "\n",
    "![SMB EXE discovery](img/hy-discovery.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ff27de-3da0-4f0f-8b7e-6a6acb34db3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The REST API\n",
    "\n",
    "A little known feature in our products is the ability to query the REST API. REST, which stands for REpresentational State Transfer, is a standard paradigm for building web applications whereby backend server is responsible for frontend components via API requests. In our case, most frontend components simply fetch and display data from backend URLs. Important part being that we have already implemented a number of useful API endpoints to fetch useful data. It's also fairly simple to add new endpoints.\n",
    "\n",
    "But before we can discuss newly added endpoints or even how anyone could contribute to adding them, we must first explore how API queries work. In short, anyone with proper *API token* is able to issue authenticated requests to endpoints. To generate that token, we must first navigate to `Account Settings` section which is available at the top right corner of the title menu.\n",
    "\n",
    "![Account Settings](img/account-settings.png)\n",
    "\n",
    "Then on the left hand side, choose `Edit Token`.\n",
    "\n",
    "![Edit Token](img/edit-token.png)\n",
    "\n",
    "Finally, the token will be visible in the `Token` field. If empty, then simply click `Regenerate` button to create a new one. Then copy the value to a keychain or password safe of your choice.\n",
    "\n",
    "![Generate Token](img/generate-token.png)\n",
    "\n",
    "Once we have found our token, we can start issuing queries to Scirius REST API. We can even fetch data from the command line! Simply point you web client to the appliance IP or fully qualified domain name with API endpoint in the *URI path*. API token must be defined within the `Authorization` header.\n",
    "\n",
    "```bash\n",
    "curl -XGET \"https://$SELKS_OR_SSP/rest/rules/es/alerts_count/\" \\\n",
    "    -H \"Authorization: Token $TOKEN\" \\\n",
    "    -H 'Content-Type: application/json'\n",
    "```\n",
    "\n",
    "This very simple endpoint returns the number of alerts that match within given time period. If left undefined, it will default to 30 days in the past to now.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"prev_doc_count\": 0,\n",
    "  \"doc_count\": 810605\n",
    "}\n",
    "```\n",
    "\n",
    "We can pull data directly from any SELKS or SSP instance. Directly from command line. That's pretty cool! But let's look at something more powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb5c21-43f0-4835-ab5b-3d609162102c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scirius REST API with Python\n",
    "\n",
    "Firstly, we need to point our notebooks to the right host. We also need to store the authentication token along with any parameters that might alter the connection. After all, hard coding variables like this into each notebook will severely diminish their usability. And to make matters worse, committing and pushing API tokens is a security breach. To keep things simple, we decided to use `.env` files. In fact, our SELKS on docker setup uses the same method, so it was only natural to use it for notebooks as well. It can be set up as described in [Suricata Analytics main README file](https://github.com/StamusNetworks/suricata-analytics/tree/main#jupyter).\n",
    "\n",
    "```bash\n",
    "SCIRIUS_TOKEN=<TOKEN VALUE>\n",
    "SCIRIUS_HOST=<IP or Hostname>\n",
    "SCIRIUS_TLS_VERIFY=yes\n",
    "```\n",
    "\n",
    "For now we handle a very limited set of options. Those being the token value itself, server IP or hostname, and an option to disable TLS verification if using self-signed certificates. Latter being the default for most lab setups and out of the box SELKS installations.\n",
    "\n",
    "Python has`dotenv` package to import variables in this file into python session. Once imported, `dotenv_values` allows us to use variables in environment file like any other python dictionary. Note that Suricata Analytics project includes a reference docker container which mounts the environment file from project root directory into the home folder of container. Subsequent example is written with this in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62267f6c-c6bd-4bde-836c-3e4cddcd04b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd404db4-4d64-405a-9461-b33b558aeedd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG = dotenv_values(\"../../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f951961-cbfc-4a71-86b2-3aa6464280a5",
   "metadata": {},
   "source": [
    "We can use Python `requests` package to interact with Scirius REST API. But before we do, we need to set up some parameters. Like before, the API token is passed with `Authorization` header. Though this time it's more structured. We can also use the environment dictionary to dynamically build the URL and authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5783b96f-f3a2-4a8d-bffc-7a3e2214a7b2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "URL = \"https://{host}/rest/rules/es/events_tail\".format(host=CONFIG[\"SCIRIUS_HOST\"])\n",
    "HEADERS = {\n",
    "    \"Authorization\": \"Token {token}\".format(token=CONFIG[\"SCIRIUS_TOKEN\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1978950e-19b8-4c30-87d3-881b8e4d8f8e",
   "metadata": {},
   "source": [
    "Each API endpoint usually defines it's own parameters. But some are common for most. The important ones being:\n",
    "* `qfilter` for passing a KQL style query to the endpoint;\n",
    "* `from_date` unix epoch to define point in time from which we want to retrieve the events;\n",
    "* `to_date` unix epoch to define point in time to which the data should be retrieved;\n",
    "* `page_size` how many documents should be fetched;\n",
    "\n",
    "Note that we can pass any Kibana style query to the endpoint using the `qfilter` parameter. Essentially allowing us to fetch any data we want. We can also modify the query the query period. The default is to fetch data from last 30 days. This is something to be careful with since many queries might match more documents than what's returned by Elasticsearch. A wide query over past 30 days with default page size would return a tiny sample of overall data, and would thus not be very useful.\n",
    "\n",
    "Ideally, we would need to fetch something specific. For example, we might be interested in `http` events where HTTP URI contains a command injection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34555390-14c3-4df2-bb55-2038bfab6078",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c9126d-63d7-4f1f-a365-8620680ae58b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from_date = datetime.now(timezone.utc) - timedelta(days=365)\n",
    "to_date = datetime.now(timezone.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff869493-e208-4cb5-8e51-467a416c64f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GET_PARAMS = {\n",
    "    \"qfilter\": \"event_type: alert AND alert.signature: *cobalt*\",\n",
    "    \"page_size\": 100,\n",
    "    \"from_date\": int(from_date.strftime('%s')) * 1000,\n",
    "    \"to_date\": int(to_date.strftime('%s')) * 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4471a981-9a9b-4239-bf73-12a1e2b82c96",
   "metadata": {},
   "source": [
    "Most data can simply be fetched with HTTP GET requests. A very powerful API endpoint to get started with is `events_tail` which allows the user to query raw EVE events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688d4365-6e2c-4170-8fb5-01885de7d6cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b753bb9-edcb-4b64-9fd7-3f4d50794b23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = requests.get(URL,\n",
    "                    headers=HEADERS,\n",
    "                    verify=False if CONFIG[\"SCIRIUS_TLS_VERIFY\"] == \"no\" else True,\n",
    "                    params=GET_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6278eb2-b4c8-4230-9175-520cf1e21e2d",
   "metadata": {},
   "source": [
    "Once the data is retrieved, we can simply load the values from `results` JSON key and pass them to Pandas `json_normalize` helper to build a flat dataframe of EVE events. Once done, we can interact with the data as described in previous posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84277dce-726f-40e6-a0ec-b5d028c276dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3085fb-9b9a-4735-afb9-2003252f01d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DF = pd.json_normalize(json.loads(resp.text)[\"results\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30953681-8927-493e-b9bd-25704a55543c",
   "metadata": {},
   "source": [
    "We can simply measure how many events were fetched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98f9b4a-b111-4200-ac57-56c32dc7b927",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9b88fb-7463-4954-b603-29f6b17c28c6",
   "metadata": {},
   "source": [
    "Or we could subset the data frame for a quick glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41f3b1e-8024-4bf5-87e8-64f8e697ce26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    DF\n",
    "    [[\"timestamp\", \"src_ip\", \"dest_ip\", \"event_type\", \"flow_id\", \"alert.signature\", \"http.hostname\", \"flow_id\"]]\n",
    "    .head(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c70fc9-90ff-40cc-9565-21ef77f69b1f",
   "metadata": {},
   "source": [
    "Naturally, a more useful interaction would be some kind of aggregate report. For example, we could see what URL-s were accessed, what user agents were used for individual HTTP hosts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dc3c83-b4d9-42fe-9eb4-3b5229a9320f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    DF\n",
    "    .groupby([\"http.hostname\"])\n",
    "    .agg({\n",
    "        \"src_ip\": \"nunique\",\n",
    "        \"dest_ip\": \"nunique\",\n",
    "        \"http.hostname\": \"unique\",\n",
    "        \"http.url\": \"unique\",\n",
    "        \"http.http_user_agent\": \"unique\"\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6098e3-1e5a-4ac4-a44f-bfeec43fd6f8",
   "metadata": {},
   "source": [
    "This is really powerful but involves some some boilerplate. In the next section we'll see how Suricata Analytics improves on this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b63132-cfc3-47bf-95b5-a6750e566e1d",
   "metadata": {},
   "source": [
    "### Suricata Analytics data connector\n",
    "\n",
    "Boilerplate refers to code that repeats in many parts of the code with little variation. But it must be there to set up some other functionality. In our case, user would need to import the API token and Scirius server address in every notebook using `dotenv`. If we ever changed how they are stored, then every notebook would break. Secondly, we would need to import requests and set up HTTP query parameters all the time.\n",
    "\n",
    "Notebooks can become really complex. Especially when weighed down with code that's actually not relevant for exploring data. Having discarded many notebooks for that reason, we decided to write a Python *data connector* to move this complexity from notebooks to importable library. This connector is also part of the Suricata Analytics project and can simply be installed with `pip install .` while in the project root directory. This idea was very much inspired by [MSTIC Jupyter and Python Security Tools](https://msticpy.readthedocs.io/en/latest/), developed by [Microsoft Threat Intelligence team (MSTIC)](https://www.microsoft.com/en-us/security/blog/topic/threat-intelligence/?sort-by=newest-oldest&date=any). Like our project, it provides data connectors to quickly import and analyze security data into Jupyter Notebooks.\n",
    "\n",
    "Once installed, the connector can be imported into any notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba986b7f-3f5e-4d6c-8ccf-a94c7b3e4deb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from surianalytics.connectors import RESTSciriusConnector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e54e1a-8deb-4dad-87dc-e8935fe268c8",
   "metadata": {},
   "source": [
    "Then we create new connector object. Environment file is automatically detected on object initialization, though the user can override the parameters with object arguments as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc4a33b-544b-4684-a82f-eda6e2f8aa71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONNECTOR = RESTSciriusConnector()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c3d674-faf9-4a63-a5bb-c99d196b72fa",
   "metadata": {},
   "source": [
    "The object maintains persistent state so the user only needs to set certain parameters once. Page size parameter is one that could be easily overlooked. User might execute one query with modified page size yet forget to pass that argument in the next. That could skew the results since the second data fetch might be partial, due to more documents matching the query than would be returned by Elasticsearch.\n",
    "\n",
    "The object allows user to simply set the parameter once. All subsequent queries would then use the value until it's once again updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d82f9b-adb5-49fa-911a-6f67e2c51890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONNECTOR.set_page_size(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2a34fe-f5dc-42dd-9e6c-4f6d9c3afceb",
   "metadata": {},
   "source": [
    "Same is true for defining the query time period. Relative time queries are very common when working with NSM data. Most users simply need to know what happened X amount of time ago in the past, and might not really care for setting exact timestamps.\n",
    "\n",
    "We provided a helper method that handles this calculation automatically. Likewise, the time frame will apply to all subsequent queries once set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca66b46-4a21-49ab-877b-e1e1c5c22b48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONNECTOR.set_query_delta(hours=365 * 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b5a2b-9c05-49fc-b92e-c67b078b92b8",
   "metadata": {},
   "source": [
    "Naturally, the user could also explicitly set from and to timestamps as RFC3339 formatted strings, a unix Epochs, or parsed Python timestamp objects. Our library handles basic validation such as ensuring that timestamps are not in reverse. \n",
    "\n",
    "These are just some of the ways how we can easily prepare the following method call. That call would then be functionally identical to `requests` example that was shown in prior section, albeit with less lines of code. We also do not need to worry about parsing the results. Our library automatically converts the resulting JSON into a normalized pandas data frame, further reducing redundant code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4427698-8584-4d3c-896f-369be14d617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = CONNECTOR.get_events_df(qfilter=\"event_type: alert AND alert.signature: *cobalt*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e94f17-ee42-45ad-974f-6d286cd4a2fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    DF\n",
    "    [[\"timestamp\", \"src_ip\", \"dest_ip\", \"event_type\", \"flow_id\", \"alert.signature\", \"http.hostname\", \"flow_id\"]]\n",
    "    .head(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b2ff3-fbfd-4d75-9fa8-a35b7fd05cb9",
   "metadata": {},
   "source": [
    "## EVE JSON - Hunting without signatures\n",
    "\n",
    "A common misconception is that Suricata is IDS that is only useful for rule based detection. That is actually not the case. Suricata has a large number of protocol parsers, each of which is able to emit JSON logs. Suricata was built from the ground up to be protocol aware. In the ancient times, IDS rules had to be written for the entire packet payload. That was one of the innovations in Suricata. Instead of dealing with entire packet, suricata would parse commonly used fields and allow the rule writer to specify a buffer. For example, if rulewriter wanted to match on HTTP user agent, she would not need to deal with entire HTTP header. Content match could be specified to only apply on user-agent buffer. Not a big leap to log a buffer that's already parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed7b0f8-a64f-4464-8ee9-8673e0609018",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FIELDS = [f for f in CONNECTOR.get_unique_fields() if not any(s in f for s in [\"stats\", \"statistics\", \"metrix\", \"host_id\", \"@\", \"geoip\", \"target.src\", \"target.dest\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046e4267-3a14-41e5-81b1-6a416483538e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(FIELDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e42d4f7-c554-451d-a6a9-71a4aa0247bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FIELDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b6592-4d6b-4977-82a5-b76f61b87568",
   "metadata": {},
   "source": [
    "But, does one need a signature match to log those fields? Nope. Protocol logger simply needs to be enabled. Once enough data is parsed, Suricata will emit a protocol event in JSON format. That JSON is called EVE (Extensible Event Format). Protocol of this event is marked under `event_type` field. Note that *alert* is only one of many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aee059-1da6-4fe5-b6a7-82947ddbea57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[e for e in CONNECTOR.get_eve_unique_values(counts=\"no\", field=\"event_type\") if not any(s in e for s in [\"stamus\", \"aggregate\", \"metrix\", \"stats\"]) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccdd02d-83f8-4854-8599-aac8790d7317",
   "metadata": {},
   "source": [
    "To illustrate this point, let's run a query for `http` event type and search for a bad malware delivery sample. For now, we only want to investigate EVE event structure and common Red Team mistakes. So we only need a few samples. For that we'll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92c08e9-fb98-492c-8df2-1df31cbebce2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONNECTOR.set_page_size(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6184af4-288e-40f8-ae85-c0a375ee1bff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087a072b-a648-4c22-a1fc-7b1ba64037f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "JSON(CONNECTOR.get_events_tail(qfilter=\"event_type: http AND http.http_user_agent: *wget* AND http.url: *.sh\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc53bb1c-3215-4d74-9a8a-7f57966820d3",
   "metadata": {},
   "source": [
    "Note that if we investigate `metadata.flowbits` field, we might oberve that a flow has already been marked by signatures. That's an important concept - signature can emit alert, but it can also flag traffic. But nevertheless, we could turn off the detection engine entirely and still be able to threat hunt by running queries on protocol logs.\n",
    "\n",
    "But there's more. Literally. Suricata can and will emit multiple events for a single flow. For example, what does a web server respond with to a HTTP request? A file, of course. Therefore, Suricata would emit HTTP event type once the request is parsed. The response would generate a `fileinfo` event with all metadata about what was returned. In this case, it's actually a shell script. Furthermore, `http` section that was visible in prior event type should also be visible here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfde1b9-009a-43b5-98db-7b1c7de0d8c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "JSON(CONNECTOR.get_events_tail(qfilter=\"event_type: fileinfo AND http.http_user_agent: *wget* AND http.url: *.sh\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04446e9b-9a48-4039-b778-504ff200cf37",
   "metadata": {},
   "source": [
    "### Event Types and Flow Correlation \n",
    "\n",
    "Now comes really important concept - **flow correlation**. Each distinct event emitted by Suricata for the same flow shares `flow_id` value. That can easily be used to link the events and to build a event history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a296fc-cb4e-4641-bd52-03fe750faa18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA = (\n",
    "    CONNECTOR\n",
    "    .set_page_size(10)\n",
    "    .get_events_tail(qfilter=\"flow_id: 805522171050078\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f605a3-6cb3-472b-bae1-7bd25ad0faa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "JSON(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb9b49-202d-4aa8-87de-0d71fc2096a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DF = (\n",
    "    pd\n",
    "    .json_normalize(DATA)\n",
    "    .sort_values(by=[\"timestamp\"], ascending=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5b6f4f-de92-4270-9536-cf6d6285e847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea98585-4097-4456-9ee3-d22d3b220ca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.html.use_mathjax = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a4290-29a8-4e0b-9e28-6cda0ed694a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if len(DF) > 0:\n",
    "    DF[[\"src_ip\", \"src_port\", \"dest_ip\", \"dest_port\", \"event_type\"] + sorted([c for c in list(DF.columns.values) if c.startswith(\"alert\") or c.startswith(\"http\") or c.startswith(\"fileinfo\")])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb1322-2948-4df1-a9f6-4635df89c39c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e8f499-fcc8-4a1c-9d53-9040441a491c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if len(DF) > 0:\n",
    "    print(list(DF[\"http.http_response_body_printable\"].dropna())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a60b72-3189-4d18-a2bf-d4e1f6b4c874",
   "metadata": {},
   "source": [
    "### Hunting unique values\n",
    "\n",
    "A powerful hunting technique is simply to keep an eye out for unique values. For example, we might luck out with something interesting when investigating TLS SNI values that originate from a tagged asset with internet destination. With this we might pick up initial delivery addresses or C2 traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fedbf5-58cf-4d68-8aab-c11c5cbf210b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    CONNECTOR\n",
    "    .get_eve_unique_values(qfilter=\"event_type: tls AND tls.sni: * AND target.src.actor_id: bt AND NOT target.dest.actor_id: bt\", \n",
    "                           field=\"tls.sni\", \n",
    "                           counts=\"no\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6123830d-905c-4cb0-8840-f1def037d6b7",
   "metadata": {},
   "source": [
    "This can be tricky, as there will be a lot of noise. But we did luck out with something interesting - someone was reading a hacking guide *on the target host*. A major opsec no-no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e5cef-fd04-4713-b6d1-e017d05901b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "JSON(CONNECTOR.get_events_tail(qfilter=\"tls.sni: level23hacktools.com\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc4ca4-9c8f-45d7-b874-203a608e427c",
   "metadata": {},
   "source": [
    "Of course, often the red teamers simply neglegt to set up proper delivery infrastructure and simply pull malware from IP. This pops out and essentially burns their infrastructure. We can check it by simply doing a same kind of query but for outbound HTTP requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eb1ad5-1314-475a-9231-c0a793ad4f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    CONNECTOR\n",
    "    .get_eve_unique_values(qfilter=\"event_type: http AND target.src.actor_id: bt AND NOT target.dest.actor_id: bt\", \n",
    "                           field=\"http.hostname\", \n",
    "                           counts=\"no\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dfc459-52ad-4c2a-b069-aa9ad8bfaf66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "JSON(CONNECTOR.get_events_tail(qfilter=\"src_ip: 10.107.99.112 AND event_type: fileinfo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b47b8f-7279-4ac6-9457-dcf536799a14",
   "metadata": {},
   "source": [
    "### SMB\n",
    "\n",
    "Another case we should explore is SMB data. As mentioned before, Suricata emits multiple EVE events for the same flow. SMB takes this to the extreme - each SMB function call becomes a distinct EVE message. For example, let's list unique `flow_id` values for SMB events inside blue team networks. But this time, let's also check document counts to see how many events actually match a single flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff39077-9afc-476b-bf14-3e8ce75297f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    CONNECTOR.get_eve_unique_values(qfilter=\"event_type: smb AND target.src.actor_id: bt\", \n",
    "                                    field=\"flow_id\", \n",
    "                                    counts=\"yes\", \n",
    "                                    size=20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80d55b8-b0f3-4a44-baf4-31eb2777adc7",
   "metadata": {},
   "source": [
    "Note that a single SMB flow can produce thousands, if not tens of thousands SMB messages. Why? Because a SMB session is acutally a sequence of commands and responses. One one hand, Suricata produces a lot of useful data to really understand what happens in a SMB session. On the other hand, it produces so much data that it's hard to find a good place to start exploring. In fact, we've come to a conclusion that most people are not looking at this data at all...\n",
    "\n",
    "Those commands and responses are a good place to start off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd1b04c-e536-43a0-b216-b3d330a2e00c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONNECTOR.get_eve_unique_values(qfilter=\"event_type: smb AND target.src.actor_id: bt\", \n",
    "                                field=\"smb.command\", \n",
    "                                counts=\"yes\", \n",
    "                                size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3205078-3cb0-43da-853e-583da47bdcd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONNECTOR.get_eve_unique_values(qfilter=\"event_type: smb AND target.src.actor_id: bt\", \n",
    "                                field=\"smb.status\", \n",
    "                                counts=\"yes\", \n",
    "                                size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca27ddc-b434-4b62-9cf3-cc8dd46d29f5",
   "metadata": {},
   "source": [
    "We find that listing unique SMB status values (responses from the server) can reveal a lot of really interesting events. For example, we have statuses like `STATUS_SHARING_VIOLATION`, `STATUS_BUFFER_OVERFLOW`, `STATUS_ACCESS_DENIED`, etc. Those are all interesting events. Furthermore, we have total gibberish statuses like `UNKNOWN_00_0000`, which actually means a status that's so rare that Suricata is actually unaware of the code. And that definitely warrants investigation.\n",
    "\n",
    "For now, let's drill down on authentication failures to see if we can catch some RT brute force attempts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a8aa48-af77-4dcf-b65b-4c5fabd09049",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONNECTOR.get_eve_unique_values(qfilter=\"event_type: smb AND target.src.actor_id: bt AND smb.status: STATUS_ACCESS_DENIED\", \n",
    "                                field=\"flow_id\", \n",
    "                                counts=\"yes\", \n",
    "                                size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e7fdd0-fb95-49a8-8666-580cfcebc9c6",
   "metadata": {},
   "source": [
    "We should instantly find plenty to investigate. Now let's investigate one of those flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f62c1c-432d-4ea9-9df1-d60021badd6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DF_SMB = (\n",
    "    CONNECTOR\n",
    "    .set_page_size(10000)\n",
    "    .get_events_df(qfilter=\"flow_id: 5854994698319\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea52a266-191b-4d0a-ab71-8ed0a3817f5a",
   "metadata": {},
   "source": [
    "A quick peek will show us that the data is quite noisy. Lot of fields to investigate, too many in fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82bd400-44b9-4d9b-acdb-5114ca7abf5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DF_SMB.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793ef8d8-f56d-41d9-ac2c-7607d4e82044",
   "metadata": {},
   "source": [
    "So, let's see what SMB fields are actually emitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba768f1c-008a-48bf-ac49-244fba4cfb59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[c for c in list(DF_SMB.columns.values) if c.startswith(\"smb.\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f0a8a3-16c5-481e-8301-c03bf74b0dee",
   "metadata": {},
   "source": [
    "Now let's do a selection of some interesting columns. It should paint a fairly nice picture about what is happening. In the selected case, we should be able to clearly see a bruteforce pattern with quite a lot to unpack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4812107-a8d6-4502-beb8-28828abee295",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "(\n",
    "    DF_SMB[[\"timestamp\", \n",
    "            \"src_ip\", \n",
    "            \"dest_ip\",\n",
    "            \"target.src.id\",\n",
    "            \"target.dest.id\",\n",
    "            \"smb.command\", \n",
    "            \"smb.status\", \n",
    "            \"smb.ext_status.severity\",\n",
    "            \"smb.named_pipe\",\n",
    "            \"smb.ntlmssp.user\",\n",
    "            \"smb.ntlmssp.host\",\n",
    "            \"smb.ntlmssp.domain\",\n",
    "            \"smb.filename\",\n",
    "            \"smb.client_dialects\",\n",
    "            \"smb.request.native_os\",\n",
    "            \"smb.response.native_os\"]]\n",
    "    .sort_values(by=[\"timestamp\"], ascending=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c328f70-c76c-4b3d-8e8b-7a71fb91256b",
   "metadata": {},
   "source": [
    "Exploring this data per event is difficult. We can drill down into a single flow with under 100 messages pretty easily, but that's only one flow of many. And a flow with thousands of events is going to be really tricky to investigate.\n",
    "\n",
    "One trick I like to use is graph representation. Idea is to connect unique SMB commands with statuses. Both are low cardinality fields and they are present in every single SMB message. So we can easily build a birds eye view of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb89dd1-dad0-48c6-9df2-d56056f36431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "JSON(CONNECTOR.get_eve_fields_graph(qfilter=\"flow_id: 5854994698319\", \n",
    "                                    col_src=\"smb.command\", \n",
    "                                    col_dest=\"smb.status\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a6e08a-4eb4-4dc0-9525-d8ee11a88ccb",
   "metadata": {},
   "source": [
    "This is a raw node and edge listing of a graph. The edges for this flow already reveal a lot of what is going on. Note how the failures top everything else. A data table in Kibana or Splunk can easily draw a nice overview of commands in relation to responses. But the data returned by this API endpoint is meant to build an actual graph.\n",
    "\n",
    "This approach greatly condenses the amount of info we need to process. The graph would not be much bigger if a flow had 10000 distinct events. We can now expand this idea further - let's build a graph of statuses in relation to commands for all SMB traffic originating from a blue team asset. While the resulting graph is a bit more convoluted, we should still see strange connections pretty clearly. Furthermore, if this was real traffic, then it's a great profiling technique. Most workstations should use SMB resources pretty much the same way. Any new connection is an anomaly which should be investigated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc609a3-7b23-42d7-9532-81f08541d3e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = (\n",
    "    CONNECTOR\n",
    "    .get_eve_fields_graph_nx(qfilter=\"event_type: smb AND target.src.actor_id: bt\",\n",
    "                             col_src=\"smb.command\", \n",
    "                             col_dest=\"smb.status\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fd8aae-0852-40d2-8d43-63fe326d5cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import holoviews as hv\n",
    "import hvplot.networkx as hvnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81852687-039a-4198-95c6-3419d1f8f4ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6673bf61-4289-4779-aef5-fb325a48d73e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1b8801-53b9-4cbb-8f90-fc329562b66c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate layout\n",
    "pos = nx.layout.spring_layout(g)\n",
    "\n",
    "width = 1600\n",
    "height = 1600\n",
    "\n",
    "# locate source nodes\n",
    "n_src = [i for i, (_, a) in enumerate(g.nodes(data=True)) if a[\"kind\"] == \"source\"]\n",
    "# locate destination nodes\n",
    "n_dst = [i for i, (_, a) in enumerate(g.nodes(data=True)) if a[\"kind\"] == \"destination\"]\n",
    "\n",
    "# generate nodes per kind\n",
    "nodes_src = hvnx.draw_networkx_nodes(g, pos, nodelist=n_src, node_color='#A0CBE2').opts(width=width, height=height)\n",
    "nodes_dst = hvnx.draw_networkx_nodes(g, pos, nodelist=n_dst, node_color=\"Orange\").opts(width=width, height=height)\n",
    "\n",
    "# generate edges\n",
    "edges = (\n",
    "    hvnx\n",
    "    .draw_networkx_edges(g, pos)\n",
    "    .opts(width=width, height=height)\n",
    ")\n",
    "\n",
    "# overlay nodes and edges\n",
    "res = edges * nodes_src * nodes_dst\n",
    "\n",
    "labels = hvnx.draw_networkx_labels(g, pos, nodelist=n_src)\n",
    "res = res * labels\n",
    "\n",
    "labels = hvnx.draw_networkx_labels(g, pos, nodelist=n_dst)\n",
    "res = res * labels\n",
    "\n",
    "show(hv.render(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49934c4-6376-4c2c-826f-918360fba161",
   "metadata": {},
   "source": [
    "We could also investigate SMB status codes in relation to domain user. This view is limited. NTLMSSP user is not present in all messages. Only in the authentication requests. So the edges are limited. But we can nevertheless use this technique to separate users that should be investigated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641e843-4b3d-468c-8829-49ae84311562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = (\n",
    "    CONNECTOR\n",
    "    .get_eve_fields_graph_nx(qfilter=\"event_type: smb AND target.src.actor_id: bt\",\n",
    "                             col_src=\"smb.status\", \n",
    "                             col_dest=\"smb.ntlmssp.user\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2310a8ac-0444-4e37-9d37-8b44ff7008cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate layout\n",
    "pos = nx.layout.spring_layout(g)\n",
    "\n",
    "width = 1600\n",
    "height = 1600\n",
    "\n",
    "# locate source nodes\n",
    "n_src = [i for i, (_, a) in enumerate(g.nodes(data=True)) if a[\"kind\"] == \"source\"]\n",
    "# locate destination nodes\n",
    "n_dst = [i for i, (_, a) in enumerate(g.nodes(data=True)) if a[\"kind\"] == \"destination\"]\n",
    "\n",
    "# generate nodes per kind\n",
    "nodes_src = hvnx.draw_networkx_nodes(g, pos, nodelist=n_src, node_color='#A0CBE2').opts(width=width, height=height)\n",
    "nodes_dst = hvnx.draw_networkx_nodes(g, pos, nodelist=n_dst, node_color=\"Orange\").opts(width=width, height=height)\n",
    "\n",
    "# generate edges\n",
    "edges = (\n",
    "    hvnx\n",
    "    .draw_networkx_edges(g, pos)\n",
    "    .opts(width=width, height=height)\n",
    ")\n",
    "\n",
    "# overlay nodes and edges\n",
    "res = edges * nodes_src * nodes_dst\n",
    "\n",
    "labels = hvnx.draw_networkx_labels(g, pos, nodelist=n_src)\n",
    "res = res * labels\n",
    "\n",
    "labels = hvnx.draw_networkx_labels(g, pos, nodelist=n_dst)\n",
    "res = res * labels\n",
    "\n",
    "show(hv.render(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0977ec5-e434-42ca-b1bb-a93e13b2538b",
   "metadata": {},
   "source": [
    "Again, this view does not need to be an actual graph. A data table would likely work well enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaccf763-beff-4d02-88c1-d763c312fa5f",
   "metadata": {},
   "source": [
    "# Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e06938c-89a3-4c29-bf12-465643e5fd78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
